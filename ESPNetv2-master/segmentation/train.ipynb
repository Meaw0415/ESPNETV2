{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn import SegmentationModel as net\n",
    "from DataSet import CamVidDataset\n",
    "\n",
    "from d2l import torch as d2l\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import monai\n",
    "from torchcontrib.optim import SWA\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from IOUEval import iouEval\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = net.EESPNet_Seg(2)\n",
    " \n",
    "train_path = \"C:/Users/asus/Desktop/Video Substraction/DAVIS/ImageSets/480p/train.txt\"\n",
    "val_path   = \"C:/Users/asus/Desktop/Video Substraction/DAVIS/ImageSets/480p/trainval.txt\"\n",
    "root_path  = 'C:/Users/asus/Desktop/Video Substraction/DAVIS'\n",
    "\n",
    "\n",
    "train_dataset = CamVidDataset(train_path,root_path)\n",
    "val_dataset = CamVidDataset(val_path,root_path)\n",
    "\n",
    " \n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True,drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True,drop_last=True)\n",
    "# training loop 100 epochs\n",
    "epochs_num = 100\n",
    "# 选用SGD优化器来训练\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "schedule = monai.optimizers.LinearLR(optimizer, end_lr=0.05, num_iter=int(epochs_num*0.75))\n",
    "# 使用SWA优化 来提升SGD的效果\n",
    "\n",
    "steps_per_epoch = int(len(train_loader.dataset) / train_loader.batch_size)\n",
    "swa_start = int(epochs_num*0.75)\n",
    "optimizer = SWA(optimizer, swa_start=swa_start*steps_per_epoch, swa_freq=steps_per_epoch, swa_lr=0.05)\n",
    " \n",
    "# 损失函数选用多分类交叉熵损失函数\n",
    "lossf = nn.CrossEntropyLoss(ignore_index=255)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()  # Set the model to evaluation mode\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # No. of correct predictions, no. of predictions\n",
    "    metric = d2l.Accumulator(2)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # Required for BERT Fine-tuning (to be covered later)\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            output = net(X)\n",
    "            pred = output[0]\n",
    "            metric.add(d2l.accuracy(pred, y), d2l.size(y))\n",
    "    return metric[0] / metric[1]\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train_ch13(output_dir,net, train_iter, test_iter, loss, optimizer, num_epochs, schedule, swa_start=swa_start, devices=d2l.try_all_gpus()):\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1], legend=['train loss', 'train acc', 'test acc'])\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    # 用来保存一些训练参数\n",
    " \n",
    "    loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    epochs_list = []\n",
    "    time_list = []\n",
    "    lr_list = []\n",
    "    mIOU_list=[]\n",
    "    total_batches = len(train_iter)\n",
    "    iouEvalTrain = iouEval(2)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples,\n",
    "        # no. of predictions\n",
    "        metric = d2l.Accumulator(4)\n",
    "        for i, (X, labels) in enumerate(train_iter):\n",
    "            a = timer.start()\n",
    " \n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(devices[0]) for x in X]\n",
    "            else:\n",
    "                X = X.to(devices[0])\n",
    "            gt = labels.long().to(devices[0])\n",
    "            # print(X.shape)\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            output1,output2 = net(X)\n",
    "            loss1 = loss(output1, gt)\n",
    "            loss2 = loss(output2, gt)\n",
    "            loss_sum = loss1+loss2\n",
    "            \n",
    "            loss_sum.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            iouEvalTrain.addBatch(output1.max(1)[1].data, gt)\n",
    "\n",
    "            b = timer.stop()\n",
    "            print('[%d/%d] loss: %.3f time: %.2f' % (i, total_batches, loss_sum.item(),timer.sum() ))\n",
    "        overall_acc, per_class_acc, per_class_iu, mIOU = iouEvalTrain.getMetric()\n",
    "        loss_list.append(loss_sum.item())\n",
    "        train_acc_list.append(overall_acc)\n",
    "        mIOU_list.append(mIOU)\n",
    "        if optimizer.state_dict()['param_groups'][0]['lr']>0.05:\n",
    "            schedule.step()\n",
    " \n",
    "        \n",
    "        \n",
    "        if (epoch + 1) >= swa_start:\n",
    "            if epoch == 0 or epoch % 5 == 5 - 1 or epoch == num_epochs - 1:\n",
    "                # Batchnorm update\n",
    "                optimizer._reset_lr_to_swa()\n",
    "                optimizer.swap_swa_sgd()\n",
    "                optimizer.bn_update(train_iter, net, device='cuda')\n",
    "                \n",
    "                optimizer.swap_swa_sgd()\n",
    "        \n",
    "        animator.add(epoch + 1, (None, None, overall_acc))\n",
    " \n",
    "        print(f\"epoch {epoch+1}/{epochs_num} --- train acc {overall_acc:.3f} --- mIOU {mIOU:.3f} --- cost time {timer.sum()}\")\n",
    "        \n",
    "        #----------------保存模型------------------- \n",
    "        if np.mod(epoch+1, 5) == 0:\n",
    "            torch.save(net.state_dict(), output_dir+ f'checkpoints/ESPNetv2_{epoch+1}.pth')\n",
    " \n",
    "    # 保存下最后的model\n",
    "    torch.save(net.state_dict(), output_dir +  f'checkpoints/ESPNetv2_last.pth')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"C:/Users/asus/Desktop/Video Substraction/Output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list,acc_list,mIOU_list = train_ch13(output_dir,model, train_loader, val_loader, lossf, optimizer, epochs_num, schedule=schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import profile\n",
    "model = model.cuda()\n",
    "input = torch.randn(16, 3,448, 448).cuda() # (batch_size, num_channel, Height, Width)\n",
    "flops, params = profile(model, inputs=(input, )) \n",
    "print('flops: {}, params: {}'.format(flops, params))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
